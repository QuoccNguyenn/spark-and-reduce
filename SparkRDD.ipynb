{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled12.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OTSrBZH5SFO"
      },
      "source": [
        "# **SPARK RDD**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yniJV9JQ5Nx_",
        "outputId": "82e4706c-de4a-4942-c0a0-3a57ca5b357d"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 47.2MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=560a3cffa9939fc02a30769e02c16e5d8f35eabdacf5de42f5207eb2281a9cbf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvXA9Eq36NTC"
      },
      "source": [
        "### **CREATE SPARK CONTEXT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-T6lnFXz5-4H"
      },
      "source": [
        "from pyspark import SparkContext\r\n",
        "sc = SparkContext(\"local\", \"count app\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hyqNILI5Rqq"
      },
      "source": [
        "words = sc.parallelize (\r\n",
        "   [\"scala\", \r\n",
        "   \"java\", \r\n",
        "   \"hadoop\", \r\n",
        "   \"spark\", \r\n",
        "   \"akka\",\r\n",
        "   \"spark vs hadoop\", \r\n",
        "   \"pyspark\",\r\n",
        "   \"pyspark and spark\"]\r\n",
        ")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z-TO42A6TiP"
      },
      "source": [
        "### **COUNT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtZVoLyl53__",
        "outputId": "c6279da7-72dd-4af3-fddb-8b3105fadb01"
      },
      "source": [
        "counts = words.count()\r\n",
        "print(\"Number of elements in RDD -> %i\" % (counts))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of elements in RDD -> 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9usktCEZ6WqV"
      },
      "source": [
        "### **COLLECT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF-iYyPz6DZj",
        "outputId": "08cdfcd7-6c92-448b-ac00-7830f2c1e7ad"
      },
      "source": [
        "coll = words.collect()\r\n",
        "print (\"Elements in RDD -> %s\" % (coll))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elements in RDD -> ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68BJ2hDR7Jt3"
      },
      "source": [
        "### **FOREACH**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-2wCMc96ips"
      },
      "source": [
        "words.foreach(lambda x: print(x))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqSVyVAm7TFH"
      },
      "source": [
        "### **FILTER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XU9QwoAb7NTc",
        "outputId": "34956812-f70e-4ef4-d96d-bc3885e98dfe"
      },
      "source": [
        "words_filter = words.filter(lambda x: 'spark' in x)\r\n",
        "filtered = words_filter.collect()\r\n",
        "print (\"Fitered RDD -> %s\" % (filtered))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitered RDD -> ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}