{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ50NUpOz1RW",
        "outputId": "33ae30fc-3aab-4663-f696-848638990a9c"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2MB 68kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 48.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=b8415495d14e9ea1baa9485ff7a7a1dfb195d39f08612d2ee4a977176cb70dab\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0mVpHdQ_Kdt"
      },
      "source": [
        "import pyspark\r\n",
        "from pyspark import  SparkConf,SparkContext\r\n",
        "import collections\r\n",
        "\r\n",
        "conf = SparkConf().setMaster(\"local\").setAppName(\"wordcounting\")\r\n",
        "\r\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZxbZZMuD8S0",
        "outputId": "6b4330f9-d191-4b9b-b574-cdcde09a3baf"
      },
      "source": [
        "text = \"to be or not to be\".split()\r\n",
        "rdd = sc.parallelize(text)\r\n",
        "counts = rdd.map(lambda word: (word, 1))\r\n",
        "print(counts.collect())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('to', 1), ('be', 1), ('or', 1), ('not', 1), ('to', 1), ('be', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Wx9ta0GXKJ",
        "outputId": "07d76c28-14b7-434b-8101-f5385f456625"
      },
      "source": [
        "red  = counts.reduceByKey(lambda x,y: x + y)\r\n",
        "print(red.collect())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('to', 2), ('be', 2), ('or', 1), ('not', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6MvuT63HsvM",
        "outputId": "360fdec7-df3b-4212-be2f-3840e7331e5f"
      },
      "source": [
        "import functools\r\n",
        "from functools import reduce \r\n",
        "# initializing list \r\n",
        "lis = [ 1 , 3, 5, 6, 2] \r\n",
        "n = len(lis)\r\n",
        "# using reduce to compute maximum element from list \r\n",
        "print (\"The maximum element of the list is : \",end=\"\") \r\n",
        "print (functools.reduce(lambda a,b : a if a > b else b,lis))\r\n",
        "\r\n",
        "print(\"The average number on array is: \",end=\"\")\r\n",
        "print (functools.reduce(lambda a,b : a + b ,lis)/n)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The maximum element of the list is : 6\n",
            "The average number on array is: 3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjFxJumxIyil"
      },
      "source": [
        "Design MapReduce algorithm to take a very large file of intergers and produce as output:\r\n",
        "a. The largest integer.\r\n",
        "b. The average of all the integers.\r\n",
        "c. The same set of integer but with each integer appearing only once.\r\n",
        "d. The count of the number of distinct integerin the  input."
      ]
    }
  ]
}